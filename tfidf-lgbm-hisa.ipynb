{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d14fde",
   "metadata": {
    "papermill": {
     "duration": 0.009997,
     "end_time": "2024-06-01T10:47:33.060924",
     "exception": false,
     "start_time": "2024-06-01T10:47:33.050927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deberta inference\n",
    "\n",
    "テストデータにて特徴量として利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9a6103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:47:33.081417Z",
     "iopub.status.busy": "2024-06-01T10:47:33.081028Z",
     "iopub.status.idle": "2024-06-01T10:48:54.683330Z",
     "shell.execute_reply": "2024-06-01T10:48:54.682243Z"
    },
    "papermill": {
     "duration": 81.614802,
     "end_time": "2024-06-01T10:48:54.685287",
     "exception": false,
     "start_time": "2024-06-01T10:47:33.070485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 10:47:43.633370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-01 10:47:43.633464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-01 10:47:43.773877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_AVAILABLE = True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff82cceae3cd4432b514746baec5bcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score\n",
       "0  000d118  Many people have car where they live. The thin...      3\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       "2  001ab80  People always wish they had the same technolog...      5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from glob import glob\n",
    "import gc\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "print(f\"{CUDA_AVAILABLE = }\")\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "TEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\n",
    "MODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\n",
    "EVAL_BATCH_SIZE = 1\n",
    "\n",
    "models = glob(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[0])\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "ds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=args, \n",
    "        data_collator=DataCollatorWithPadding(tokenizer), \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(ds).predictions\n",
    "    predictions.append(softmax(preds, axis=-1))  \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "predicted_score = 0.\n",
    "\n",
    "for p in predictions:\n",
    "    predicted_score += p\n",
    "    \n",
    "predicted_score /= len(predictions)\n",
    "\n",
    "df_test['score'] = predicted_score.argmax(-1) + 1\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45b8a1",
   "metadata": {
    "papermill": {
     "duration": 0.011666,
     "end_time": "2024-06-01T10:48:54.708000",
     "exception": false,
     "start_time": "2024-06-01T10:48:54.696334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a4a92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:48:54.731545Z",
     "iopub.status.busy": "2024-06-01T10:48:54.730984Z",
     "iopub.status.idle": "2024-06-01T10:48:57.577675Z",
     "shell.execute_reply": "2024-06-01T10:48:57.576515Z"
    },
    "papermill": {
     "duration": 2.860827,
     "end_time": "2024-06-01T10:48:57.580149",
     "exception": false,
     "start_time": "2024-06-01T10:48:54.719322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from tqdm.auto import tqdm,trange\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba9706",
   "metadata": {
    "papermill": {
     "duration": 0.010948,
     "end_time": "2024-06-01T10:48:57.602469",
     "exception": false,
     "start_time": "2024-06-01T10:48:57.591521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242d8085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:48:57.626345Z",
     "iopub.status.busy": "2024-06-01T10:48:57.625620Z",
     "iopub.status.idle": "2024-06-01T10:48:58.189988Z",
     "shell.execute_reply": "2024-06-01T10:48:58.189032Z"
    },
    "papermill": {
     "duration": 0.578505,
     "end_time": "2024-06-01T10:48:58.191915",
     "exception": false,
     "start_time": "2024-06-01T10:48:57.613410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th><th>paragraph</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people ha…</td><td>3</td><td>[&quot;Many people have car where they live. The thing they don&#x27;t know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban&#x27;s families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won&#x27;t see a car in Vauban&#x27;s streets because they are completely &quot;car free&quot; but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called &quot;smart planning&quot;. The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that &quot;All of our development since World war 2 has been centered on the cars,and that will have to change&quot; and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don&#x27;t need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called &quot;car reduced&quot;communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.    &quot;]</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scienti…</td><td>3</td><td>[&quot;I am a scientist at NASA that is discussing the &quot;face&quot; on mars. I will be explaining how the &quot;face&quot; is a land form. By sharing my information about this isue i will tell you just that.&quot;, &quot;First off, how could it be a martions drawing. There is no plant life on mars as of rite now that we know of, which means so far as we know it is not possible for any type of life. That explains how it could not be made by martians. Also why and how would a martion build a face so big. It just does not make any since that a martian did this.&quot;, … &quot;To sum all this up the &quot;face&quot; on mars is a landform but others would like to beleive it&#x27;s a martian sculpture. Which every one that works at NASA says it&#x27;s a landform and they are all the ones working on the planet and taking pictures.&quot;]</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always …</td><td>4</td><td>[&quot;People always wish they had the same technology that they have seen in movies, or the best new piece of technology that is all over social media. However, nobody seems to think of the risks that these kinds of new technologies may have. Cars have been around for many decades, and now manufacturers are starting to get on the bandwagon and come up with the new and improved technology that they hope will appeal to everyone. As of right now, it seems as though the negative characteristics of these cars consume the positive idea that these manufacturers have tried to convey.&quot;, &quot;Currently, this new technology in cars has a very long way to go before being completely &quot;driverless&quot;. Drivers still need to be on alert when they are driving, as well as control the car near any accidents or complicated traffic situations. This seems to totally defeat the purpose of the &quot;driverless&quot; car. Eventually the technology may improve, but nobody can be certain that the driverless car will eventually become completely &quot;driverless&quot;. This idea just seems like a lot of hard work and money for something that is not very neccessary. If someone does not want to drive their car they can just take a city bus or a subway. There are so many options of transportation that can already solve this problem. Even if masnufacturers are trying to make driving more &quot;fun&quot;, driving is not meant to be &quot;fun&quot; it is meant to get people where they need to go. Playing around in a car just to have &quot;fun&quot; is just a recipe for disaster.&quot;, … &quot;The technology car manufacturers are trying to develope may just be a diasaster in the making. There are many alternative options of transportations if you do not feel like driving yourself, and these options are way less expensive than buying a brand new car. Although this technology is relatively new, we can not be certain that this new idea will even pay off in the end, it may just be a waste of money and time. Sometimes the newest technology is not the most benefical.         &quot;]</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard a…</td><td>4</td><td>[&quot;We all heard about Venus, the planet without almost oxygen with earthquakes, erupting volcanoes and temperatures average over 800 degrees Fahrenheit but what if scientist project the futur into this planet ? Through this article, the author uses evidences appealing to reason and concession to make us realize why we should care about studying this planet so that people must give a chance to Venus.&quot;, &quot;Venus is the closest planet to Earth in terms density and size but has a really different climate. As it is evoked by the author:&quot;, … &quot;In conclusion, despite of Venus hostility put in advance by the concession, the author makes the audience realize that there&#x27;s a solution but that we can find it only if we study the planet. He make us find out that challenge and curiosity is part of human life. But also that danger and fear should not stop us from discovering new things. After all, we are Humans.&quot;]</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Se…</td><td>3</td><td>[&quot;Dear, State Senator&quot;, &quot;This is a letter to argue in favor of keeping the Electoral College.&quot;There are many reasons to keep the Electoral College&quot; one reason is because it is widely regarded as an anachronism, a dispute over the outcome of an Electoral College vote is possible, but it is less likely than a dispute over the popular vote, and the Electoral College restores some of the weight in the political balance that large states (by population) lose by virue of the mal apportionment of the Senate decreed in the Constitution.&quot;, … &quot;From, PROPER_NAME            &quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬───────────────────────────────────┬───────┬───────────────────────────────────┐\n",
       "│ essay_id ┆ full_text                         ┆ score ┆ paragraph                         │\n",
       "│ ---      ┆ ---                               ┆ ---   ┆ ---                               │\n",
       "│ str      ┆ str                               ┆ i64   ┆ list[str]                         │\n",
       "╞══════════╪═══════════════════════════════════╪═══════╪═══════════════════════════════════╡\n",
       "│ 000d118  ┆ Many people have car where they … ┆ 3     ┆ [\"Many people have car where the… │\n",
       "│ 000fe60  ┆ I am a scientist at NASA that is… ┆ 3     ┆ [\"I am a scientist at NASA that … │\n",
       "│ 001ab80  ┆ People always wish they had the … ┆ 4     ┆ [\"People always wish they had th… │\n",
       "│ 001bdc0  ┆ We all heard about Venus, the pl… ┆ 4     ┆ [\"We all heard about Venus, the … │\n",
       "│ 002ba53  ┆ Dear, State Senator               ┆ 3     ┆ [\"Dear, State Senator\", \"This is… │\n",
       "│          ┆                                   ┆       ┆                                   │\n",
       "│          ┆ This is a l…                      ┆       ┆                                   │\n",
       "└──────────┴───────────────────────────────────┴───────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paragraph列の追加\n",
    "columns = [  \n",
    "    (\n",
    "        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n",
    "    ),\n",
    "]\n",
    "PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "\n",
    "# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\n",
    "train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n",
    "test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c377b47",
   "metadata": {
    "papermill": {
     "duration": 0.010975,
     "end_time": "2024-06-01T10:48:58.214257",
     "exception": false,
     "start_time": "2024-06-01T10:48:58.203282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07b6c9",
   "metadata": {
    "papermill": {
     "duration": 0.010759,
     "end_time": "2024-06-01T10:48:58.235939",
     "exception": false,
     "start_time": "2024-06-01T10:48:58.225180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc3e7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:48:58.259614Z",
     "iopub.status.busy": "2024-06-01T10:48:58.259323Z",
     "iopub.status.idle": "2024-06-01T10:48:58.280098Z",
     "shell.execute_reply": "2024-06-01T10:48:58.279238Z"
    },
    "papermill": {
     "duration": 0.034757,
     "end_time": "2024-06-01T10:48:58.281933",
     "exception": false,
     "start_time": "2024-06-01T10:48:58.247176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cList = {\n",
    "  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "   }\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "#     x = expandContractions(x)\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43f56e",
   "metadata": {
    "papermill": {
     "duration": 0.011135,
     "end_time": "2024-06-01T10:48:58.304456",
     "exception": false,
     "start_time": "2024-06-01T10:48:58.293321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.Paragraph Features（段落の処理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de705df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:48:58.329005Z",
     "iopub.status.busy": "2024-06-01T10:48:58.328726Z",
     "iopub.status.idle": "2024-06-01T10:49:00.793034Z",
     "shell.execute_reply": "2024-06-01T10:49:00.792162Z"
    },
    "papermill": {
     "duration": 2.478872,
     "end_time": "2024-06-01T10:49:00.795480",
     "exception": false,
     "start_time": "2024-06-01T10:48:58.316608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#スペルミスのカウント\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n",
    "    english_vocab = set(word.strip().lower() for word in file)\n",
    "    \n",
    "def count_spelling_errors(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
    "    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n",
    "    return spelling_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98230b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:49:00.821512Z",
     "iopub.status.busy": "2024-06-01T10:49:00.821149Z",
     "iopub.status.idle": "2024-06-01T10:49:00.826631Z",
     "shell.execute_reply": "2024-06-01T10:49:00.825723Z"
    },
    "papermill": {
     "duration": 0.019929,
     "end_time": "2024-06-01T10:49:00.828510",
     "exception": false,
     "start_time": "2024-06-01T10:49:00.808581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#句読点の削除\n",
    "\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from the input text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with punctuation removed.\n",
    "    \"\"\"\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6473bd39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T10:49:00.852191Z",
     "iopub.status.busy": "2024-06-01T10:49:00.851912Z",
     "iopub.status.idle": "2024-06-01T11:10:48.269724Z",
     "shell.execute_reply": "2024-06-01T11:10:48.268757Z"
    },
    "papermill": {
     "duration": 1307.44462,
     "end_time": "2024-06-01T11:10:48.284352",
     "exception": false,
     "start_time": "2024-06-01T10:49:00.839732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_&gt;0_cnt</th>\n",
       "      <th>paragraph_&gt;50_cnt</th>\n",
       "      <th>paragraph_&gt;75_cnt</th>\n",
       "      <th>paragraph_&gt;100_cnt</th>\n",
       "      <th>paragraph_&gt;125_cnt</th>\n",
       "      <th>paragraph_&gt;150_cnt</th>\n",
       "      <th>paragraph_&gt;175_cnt</th>\n",
       "      <th>paragraph_&gt;200_cnt</th>\n",
       "      <th>paragraph_&gt;250_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>paragraph_word_cnt_kurtosis</th>\n",
       "      <th>paragraph_error_num_q1</th>\n",
       "      <th>paragraph_len_q1</th>\n",
       "      <th>paragraph_sentence_cnt_q1</th>\n",
       "      <th>paragraph_word_cnt_q1</th>\n",
       "      <th>paragraph_error_num_q3</th>\n",
       "      <th>paragraph_len_q3</th>\n",
       "      <th>paragraph_sentence_cnt_q3</th>\n",
       "      <th>paragraph_word_cnt_q3</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.388460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.696723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_>0_cnt  paragraph_>50_cnt  paragraph_>75_cnt  \\\n",
       "0  000d118                 1                  1                  1   \n",
       "1  000fe60                 5                  5                  5   \n",
       "2  001ab80                 4                  4                  4   \n",
       "\n",
       "   paragraph_>100_cnt  paragraph_>125_cnt  paragraph_>150_cnt  \\\n",
       "0                   1                   1                   1   \n",
       "1                   5                   5                   5   \n",
       "2                   4                   4                   4   \n",
       "\n",
       "   paragraph_>175_cnt  paragraph_>200_cnt  paragraph_>250_cnt  ...  \\\n",
       "0                   1                   1                   1  ...   \n",
       "1                   5                   4                   3  ...   \n",
       "2                   4                   4                   4  ...   \n",
       "\n",
       "   paragraph_word_cnt_kurtosis  paragraph_error_num_q1  paragraph_len_q1  \\\n",
       "0                          NaN                    27.0            2640.0   \n",
       "1                    -1.388460                     1.0             235.0   \n",
       "2                    -1.696723                     1.0             576.0   \n",
       "\n",
       "   paragraph_sentence_cnt_q1  paragraph_word_cnt_q1  paragraph_error_num_q3  \\\n",
       "0                       14.0                  491.0                    27.0   \n",
       "1                        4.0                   46.0                     1.0   \n",
       "2                        5.0                  101.0                     2.0   \n",
       "\n",
       "   paragraph_len_q3  paragraph_sentence_cnt_q3  paragraph_word_cnt_q3  score  \n",
       "0            2640.0                       14.0                  491.0      3  \n",
       "1             398.0                        5.0                   77.0      3  \n",
       "2             927.0                        8.0                  165.0      4  \n",
       "\n",
       "[3 rows x 55 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Paragraph_Preprocess(tmp):\n",
    "\n",
    "    tmp = tmp.explode('paragraph')\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n",
    "                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n",
    "paragraph_fea2 = ['paragraph_error_num'] + paragraph_fea\n",
    "def Paragraph_Eng(train_tmp):\n",
    "    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n",
    "    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n",
    "    aggs = [\n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_>{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_<{i}_cnt\") for i in [25,49]], \n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "tmp = Paragraph_Preprocess(train)\n",
    "train_feats = Paragraph_Eng(tmp)\n",
    "train_feats['score'] = train['score']\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88201918",
   "metadata": {
    "papermill": {
     "duration": 0.011301,
     "end_time": "2024-06-01T11:10:48.306983",
     "exception": false,
     "start_time": "2024-06-01T11:10:48.295682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.Sentence Features（文の処理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d53dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:10:48.331924Z",
     "iopub.status.busy": "2024-06-01T11:10:48.331357Z",
     "iopub.status.idle": "2024-06-01T11:10:55.325617Z",
     "shell.execute_reply": "2024-06-01T11:10:55.324694Z"
    },
    "papermill": {
     "duration": 7.009512,
     "end_time": "2024-06-01T11:10:55.327701",
     "exception": false,
     "start_time": "2024-06-01T11:10:48.318189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_&gt;0_cnt</th>\n",
       "      <th>paragraph_&gt;50_cnt</th>\n",
       "      <th>paragraph_&gt;75_cnt</th>\n",
       "      <th>paragraph_&gt;100_cnt</th>\n",
       "      <th>paragraph_&gt;125_cnt</th>\n",
       "      <th>paragraph_&gt;150_cnt</th>\n",
       "      <th>paragraph_&gt;175_cnt</th>\n",
       "      <th>paragraph_&gt;200_cnt</th>\n",
       "      <th>paragraph_&gt;250_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_len_first</th>\n",
       "      <th>sentence_word_cnt_first</th>\n",
       "      <th>sentence_len_last</th>\n",
       "      <th>sentence_word_cnt_last</th>\n",
       "      <th>sentence_len_kurtosis</th>\n",
       "      <th>sentence_word_cnt_kurtosis</th>\n",
       "      <th>sentence_len_q1</th>\n",
       "      <th>sentence_word_cnt_q1</th>\n",
       "      <th>sentence_len_q3</th>\n",
       "      <th>sentence_word_cnt_q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.438632</td>\n",
       "      <td>2.175806</td>\n",
       "      <td>109.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.917062</td>\n",
       "      <td>0.505776</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004393</td>\n",
       "      <td>0.270079</td>\n",
       "      <td>86.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_>0_cnt  paragraph_>50_cnt  paragraph_>75_cnt  \\\n",
       "0  000d118                 1                  1                  1   \n",
       "1  000fe60                 5                  5                  5   \n",
       "2  001ab80                 4                  4                  4   \n",
       "\n",
       "   paragraph_>100_cnt  paragraph_>125_cnt  paragraph_>150_cnt  \\\n",
       "0                   1                   1                   1   \n",
       "1                   5                   5                   5   \n",
       "2                   4                   4                   4   \n",
       "\n",
       "   paragraph_>175_cnt  paragraph_>200_cnt  paragraph_>250_cnt  ...  \\\n",
       "0                   1                   1                   1  ...   \n",
       "1                   5                   4                   3  ...   \n",
       "2                   4                   4                   4  ...   \n",
       "\n",
       "   sentence_len_first  sentence_word_cnt_first  sentence_len_last  \\\n",
       "0                  36                        7                  0   \n",
       "1                  62                       13                  0   \n",
       "2                 144                       27                  0   \n",
       "\n",
       "   sentence_word_cnt_last  sentence_len_kurtosis  sentence_word_cnt_kurtosis  \\\n",
       "0                       1               1.438632                    2.175806   \n",
       "1                       1               0.917062                    0.505776   \n",
       "2                       1              -0.004393                    0.270079   \n",
       "\n",
       "   sentence_len_q1  sentence_word_cnt_q1  sentence_len_q3  \\\n",
       "0            109.0                  19.0            225.0   \n",
       "1             51.0                  12.0            124.0   \n",
       "2             86.0                  17.0            151.0   \n",
       "\n",
       "   sentence_word_cnt_q3  \n",
       "0                  37.0  \n",
       "1                  25.0  \n",
       "2                  29.0  \n",
       "\n",
       "[3 rows x 83 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Sentence_Preprocess(tmp):\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n",
    "    tmp = tmp.explode('sentence')\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))    \n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "sentence_fea = ['sentence_len','sentence_word_cnt']\n",
    "def Sentence_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_>{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea],\n",
    "    \n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Sentence_Preprocess(train)\n",
    "train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69616672",
   "metadata": {
    "papermill": {
     "duration": 0.012543,
     "end_time": "2024-06-01T11:10:55.353896",
     "exception": false,
     "start_time": "2024-06-01T11:10:55.341353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.Word Features（単語の処理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e145b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:10:55.380490Z",
     "iopub.status.busy": "2024-06-01T11:10:55.379659Z",
     "iopub.status.idle": "2024-06-01T11:11:08.923666Z",
     "shell.execute_reply": "2024-06-01T11:11:08.922682Z"
    },
    "papermill": {
     "duration": 13.559534,
     "end_time": "2024-06-01T11:11:08.925648",
     "exception": false,
     "start_time": "2024-06-01T11:10:55.366114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_&gt;0_cnt</th>\n",
       "      <th>paragraph_&gt;50_cnt</th>\n",
       "      <th>paragraph_&gt;75_cnt</th>\n",
       "      <th>paragraph_&gt;100_cnt</th>\n",
       "      <th>paragraph_&gt;125_cnt</th>\n",
       "      <th>paragraph_&gt;150_cnt</th>\n",
       "      <th>paragraph_&gt;175_cnt</th>\n",
       "      <th>paragraph_&gt;200_cnt</th>\n",
       "      <th>paragraph_&gt;250_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>word_12_cnt</th>\n",
       "      <th>word_13_cnt</th>\n",
       "      <th>word_14_cnt</th>\n",
       "      <th>word_15_cnt</th>\n",
       "      <th>word_len_max</th>\n",
       "      <th>word_len_mean</th>\n",
       "      <th>word_len_std</th>\n",
       "      <th>word_len_q1</th>\n",
       "      <th>word_len_q2</th>\n",
       "      <th>word_len_q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>4.378819</td>\n",
       "      <td>2.538495</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.012048</td>\n",
       "      <td>2.060968</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4.574545</td>\n",
       "      <td>2.604621</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_>0_cnt  paragraph_>50_cnt  paragraph_>75_cnt  \\\n",
       "0  000d118                 1                  1                  1   \n",
       "1  000fe60                 5                  5                  5   \n",
       "2  001ab80                 4                  4                  4   \n",
       "\n",
       "   paragraph_>100_cnt  paragraph_>125_cnt  paragraph_>150_cnt  \\\n",
       "0                   1                   1                   1   \n",
       "1                   5                   5                   5   \n",
       "2                   4                   4                   4   \n",
       "\n",
       "   paragraph_>175_cnt  paragraph_>200_cnt  paragraph_>250_cnt  ...  \\\n",
       "0                   1                   1                   1  ...   \n",
       "1                   5                   4                   3  ...   \n",
       "2                   4                   4                   4  ...   \n",
       "\n",
       "   word_12_cnt  word_13_cnt  word_14_cnt  word_15_cnt  word_len_max  \\\n",
       "0            6            6            5            2            25   \n",
       "1            0            0            0            0            11   \n",
       "2           14           10            5            2            15   \n",
       "\n",
       "   word_len_mean  word_len_std  word_len_q1  word_len_q2  word_len_q3  \n",
       "0       4.378819      2.538495          3.0          4.0          5.0  \n",
       "1       4.012048      2.060968          2.0          4.0          5.0  \n",
       "2       4.574545      2.604621          3.0          4.0          5.0  \n",
       "\n",
       "[3 rows x 104 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word feature\n",
    "def Word_Preprocess(tmp):\n",
    "    # Preprocess full_text and use spaces to separate words from the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n",
    "    tmp = tmp.explode('word')\n",
    "    # Calculate the length of each word\n",
    "    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n",
    "    # Delete data with a word length of 0\n",
    "    tmp = tmp.filter(pl.col('word_len')!=0)\n",
    "    \n",
    "    return tmp\n",
    "# feature_eng\n",
    "def Word_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of words with a length greater than i+1\n",
    "        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n",
    "        # other\n",
    "        pl.col('word_len').max().alias(f\"word_len_max\"),\n",
    "        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n",
    "        pl.col('word_len').std().alias(f\"word_len_std\"),\n",
    "        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n",
    "        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n",
    "        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Word_Preprocess(train)\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854f6f5",
   "metadata": {
    "papermill": {
     "duration": 0.011868,
     "end_time": "2024-06-01T11:11:08.950024",
     "exception": false,
     "start_time": "2024-06-01T11:11:08.938156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.Tf-idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe2ce31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:11:08.975530Z",
     "iopub.status.busy": "2024-06-01T11:11:08.975251Z",
     "iopub.status.idle": "2024-06-01T11:14:27.935576Z",
     "shell.execute_reply": "2024-06-01T11:14:27.934588Z"
    },
    "papermill": {
     "duration": 199.030952,
     "end_time": "2024-06-01T11:14:27.993139",
     "exception": false,
     "start_time": "2024-06-01T11:11:08.962187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  19729\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_&gt;0_cnt</th>\n",
       "      <th>paragraph_&gt;50_cnt</th>\n",
       "      <th>paragraph_&gt;75_cnt</th>\n",
       "      <th>paragraph_&gt;100_cnt</th>\n",
       "      <th>paragraph_&gt;125_cnt</th>\n",
       "      <th>paragraph_&gt;150_cnt</th>\n",
       "      <th>paragraph_&gt;175_cnt</th>\n",
       "      <th>paragraph_&gt;200_cnt</th>\n",
       "      <th>paragraph_&gt;250_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_19617</th>\n",
       "      <th>tfid_19618</th>\n",
       "      <th>tfid_19619</th>\n",
       "      <th>tfid_19620</th>\n",
       "      <th>tfid_19621</th>\n",
       "      <th>tfid_19622</th>\n",
       "      <th>tfid_19623</th>\n",
       "      <th>tfid_19624</th>\n",
       "      <th>tfid_19625</th>\n",
       "      <th>tfid_19626</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 19731 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_>0_cnt  paragraph_>50_cnt  paragraph_>75_cnt  \\\n",
       "0  000d118                 1                  1                  1   \n",
       "1  000fe60                 5                  5                  5   \n",
       "2  001ab80                 4                  4                  4   \n",
       "\n",
       "   paragraph_>100_cnt  paragraph_>125_cnt  paragraph_>150_cnt  \\\n",
       "0                   1                   1                   1   \n",
       "1                   5                   5                   5   \n",
       "2                   4                   4                   4   \n",
       "\n",
       "   paragraph_>175_cnt  paragraph_>200_cnt  paragraph_>250_cnt  ...  \\\n",
       "0                   1                   1                   1  ...   \n",
       "1                   5                   4                   3  ...   \n",
       "2                   4                   4                   4  ...   \n",
       "\n",
       "   tfid_19617  tfid_19618  tfid_19619  tfid_19620  tfid_19621  tfid_19622  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   tfid_19623  tfid_19624  tfid_19625  tfid_19626  \n",
       "0         0.0         0.0         0.0         0.0  \n",
       "1         0.0         0.0         0.0         0.0  \n",
       "2         0.0         0.0         0.0         0.0  \n",
       "\n",
       "[3 rows x 19731 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer parameter\n",
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(3,6),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    ")\n",
    "# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n",
    "train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n",
    "# Convert to array\n",
    "dense_matrix = train_tfid.toarray()\n",
    "# Convert to dataframe\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "# rename features　\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869104df",
   "metadata": {
    "papermill": {
     "duration": 0.012258,
     "end_time": "2024-06-01T11:14:28.018072",
     "exception": false,
     "start_time": "2024-06-01T11:14:28.005814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CountVectorizer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b54919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:14:28.044670Z",
     "iopub.status.busy": "2024-06-01T11:14:28.044315Z",
     "iopub.status.idle": "2024-06-01T11:15:55.142818Z",
     "shell.execute_reply": "2024-06-01T11:15:55.141851Z"
    },
    "papermill": {
     "duration": 87.128856,
     "end_time": "2024-06-01T11:15:55.159344",
     "exception": false,
     "start_time": "2024-06-01T11:14:28.030488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  21941\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_&gt;0_cnt</th>\n",
       "      <th>paragraph_&gt;50_cnt</th>\n",
       "      <th>paragraph_&gt;75_cnt</th>\n",
       "      <th>paragraph_&gt;100_cnt</th>\n",
       "      <th>paragraph_&gt;125_cnt</th>\n",
       "      <th>paragraph_&gt;150_cnt</th>\n",
       "      <th>paragraph_&gt;175_cnt</th>\n",
       "      <th>paragraph_&gt;200_cnt</th>\n",
       "      <th>paragraph_&gt;250_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_cnt_2202</th>\n",
       "      <th>tfid_cnt_2203</th>\n",
       "      <th>tfid_cnt_2204</th>\n",
       "      <th>tfid_cnt_2205</th>\n",
       "      <th>tfid_cnt_2206</th>\n",
       "      <th>tfid_cnt_2207</th>\n",
       "      <th>tfid_cnt_2208</th>\n",
       "      <th>tfid_cnt_2209</th>\n",
       "      <th>tfid_cnt_2210</th>\n",
       "      <th>tfid_cnt_2211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_>0_cnt  paragraph_>50_cnt  paragraph_>75_cnt  \\\n",
       "0  000d118                 1                  1                  1   \n",
       "1  000fe60                 5                  5                  5   \n",
       "2  001ab80                 4                  4                  4   \n",
       "\n",
       "   paragraph_>100_cnt  paragraph_>125_cnt  paragraph_>150_cnt  \\\n",
       "0                   1                   1                   1   \n",
       "1                   5                   5                   5   \n",
       "2                   4                   4                   4   \n",
       "\n",
       "   paragraph_>175_cnt  paragraph_>200_cnt  paragraph_>250_cnt  ...  \\\n",
       "0                   1                   1                   1  ...   \n",
       "1                   5                   4                   3  ...   \n",
       "2                   4                   4                   4  ...   \n",
       "\n",
       "   tfid_cnt_2202  tfid_cnt_2203  tfid_cnt_2204  tfid_cnt_2205  tfid_cnt_2206  \\\n",
       "0              0              0              0              1              0   \n",
       "1              0              1              1              0              0   \n",
       "2              2              0              0              0              0   \n",
       "\n",
       "   tfid_cnt_2207  tfid_cnt_2208  tfid_cnt_2209  tfid_cnt_2210  tfid_cnt_2211  \n",
       "0              0              0              0              5              0  \n",
       "1              0              0              0              0              0  \n",
       "2              0              0              0              0              0  \n",
       "\n",
       "[3 rows x 21943 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_cnt = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,3),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,\n",
    ")\n",
    "train_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\n",
    "dense_matrix = train_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca38b8",
   "metadata": {
    "papermill": {
     "duration": 0.013593,
     "end_time": "2024-06-01T11:15:55.186488",
     "exception": false,
     "start_time": "2024-06-01T11:15:55.172895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deberta predictions to LGBM as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a88c5ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:15:55.216010Z",
     "iopub.status.busy": "2024-06-01T11:15:55.215649Z",
     "iopub.status.idle": "2024-06-01T11:15:55.223787Z",
     "shell.execute_reply": "2024-06-01T11:15:55.222941Z"
    },
    "papermill": {
     "duration": 0.02543,
     "end_time": "2024-06-01T11:15:55.225822",
     "exception": false,
     "start_time": "2024-06-01T11:15:55.200392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    y_true = y_true + a\n",
    "    y_pred = (y_pred + a).clip(1, 6).round()\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    return 'QWK', qwk, True\n",
    "def qwk_obj(y_true, y_pred):\n",
    "    labels = y_true + a\n",
    "    preds = y_pred + a\n",
    "    preds = preds.clip(1, 6)\n",
    "    f = 1/2*np.sum((preds-labels)**2)\n",
    "    g = 1/2*np.sum((preds-a)**2+b)\n",
    "    df = preds - labels\n",
    "    dg = preds - a\n",
    "    grad = (df/g - f*dg/g**2)*len(labels)\n",
    "    hess = np.ones(len(labels))\n",
    "    return grad, hess\n",
    "a = 2.948\n",
    "b = 1.092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c615b646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:15:55.255049Z",
     "iopub.status.busy": "2024-06-01T11:15:55.254780Z",
     "iopub.status.idle": "2024-06-01T11:15:55.306525Z",
     "shell.execute_reply": "2024-06-01T11:15:55.305634Z"
    },
    "papermill": {
     "duration": 0.068676,
     "end_time": "2024-06-01T11:15:55.308562",
     "exception": false,
     "start_time": "2024-06-01T11:15:55.239886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17307, 6) (17307, 21943)\n",
      "Features Number:  21947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17307, 21949)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# add Deberta predictions to LGBM as features\n",
    "deberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\n",
    "print(deberta_oof.shape, train_feats.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ', len(feature_names))    \n",
    "\n",
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c186f093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:15:55.337952Z",
     "iopub.status.busy": "2024-06-01T11:15:55.337033Z",
     "iopub.status.idle": "2024-06-01T11:15:57.239773Z",
     "shell.execute_reply": "2024-06-01T11:15:57.238661Z"
    },
    "papermill": {
     "duration": 1.919946,
     "end_time": "2024-06-01T11:15:57.242122",
     "exception": false,
     "start_time": "2024-06-01T11:15:55.322176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting the 'text' column to string type and assigning to X\n",
    "X = train_feats[feature_names].astype(np.float32).values\n",
    "\n",
    "# Converting the 'score' column to integer type and assigning to y\n",
    "y_split = train_feats['score'].astype(int).values\n",
    "y = train_feats['score'].astype(np.float32).values-a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ab61d",
   "metadata": {
    "papermill": {
     "duration": 0.014009,
     "end_time": "2024-06-01T11:15:57.270417",
     "exception": false,
     "start_time": "2024-06-01T11:15:57.256408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f644eb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:15:57.300678Z",
     "iopub.status.busy": "2024-06-01T11:15:57.299815Z",
     "iopub.status.idle": "2024-06-01T11:15:57.312951Z",
     "shell.execute_reply": "2024-06-01T11:15:57.312064Z"
    },
    "papermill": {
     "duration": 0.030315,
     "end_time": "2024-06-01T11:15:57.314930",
     "exception": false,
     "start_time": "2024-06-01T11:15:57.284615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_select_wrapper():\n",
    "    \"\"\"\n",
    "    lgm\n",
    "    :param train\n",
    "    :param test\n",
    "    :return\n",
    "    \"\"\"\n",
    "    # Part 1.\n",
    "    print('feature_select_wrapper...')\n",
    "    features = feature_names\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    fse = pd.Series(0, index=features)\n",
    "         \n",
    "    for train_index, test_index in skf.split(X, y_split):\n",
    "\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "                    objective = qwk_obj,\n",
    "                    metrics = 'None',\n",
    "                    learning_rate = 0.05,\n",
    "                    max_depth = 5,\n",
    "                    num_leaves = 10,\n",
    "                    colsample_bytree=0.3,\n",
    "                    reg_alpha = 0.7,\n",
    "                    reg_lambda = 0.1,\n",
    "                    n_estimators=700,\n",
    "                    random_state=412,\n",
    "                    extra_trees=True,\n",
    "                    class_weight='balanced',\n",
    "                    verbosity = - 1)\n",
    "\n",
    "        predictor = model.fit(X_train_fold,\n",
    "                              y_train_fold,\n",
    "                              eval_names=['train', 'valid'],\n",
    "                              eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n",
    "                              eval_metric=quadratic_weighted_kappa,\n",
    "                              callbacks=callbacks)\n",
    "        models.append(predictor)\n",
    "        predictions_fold = predictor.predict(X_test_fold)\n",
    "        predictions_fold = predictions_fold + a\n",
    "        predictions_fold = predictions_fold.clip(1, 6).round()\n",
    "        predictions.append(predictions_fold)\n",
    "        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n",
    "        f1_scores.append(f1_fold)\n",
    "\n",
    "        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n",
    "        kappa_scores.append(kappa_fold)\n",
    "\n",
    "#         cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n",
    "\n",
    "#         disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "#                                       display_labels=[x for x in range(1,7)])\n",
    "#         disp.plot()\n",
    "#         plt.show()\n",
    "        print(f'F1 score across fold: {f1_fold}')\n",
    "        print(f'Cohen kappa score across fold: {kappa_fold}')\n",
    "\n",
    "        fse += pd.Series(predictor.feature_importances_, features)  \n",
    "    \n",
    "    # Part 4.\n",
    "    feature_select = fse.sort_values(ascending=False).index.tolist()[:13000]\n",
    "    print('done')\n",
    "    return feature_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e702179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:15:57.344328Z",
     "iopub.status.busy": "2024-06-01T11:15:57.343691Z",
     "iopub.status.idle": "2024-06-01T11:30:46.838681Z",
     "shell.execute_reply": "2024-06-01T11:30:46.837565Z"
    },
    "papermill": {
     "duration": 889.511614,
     "end_time": "2024-06-01T11:30:46.840896",
     "exception": false,
     "start_time": "2024-06-01T11:15:57.329282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_select_wrapper...\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.769784\tvalid's QWK: 0.761228\n",
      "[50]\ttrain's QWK: 0.82354\tvalid's QWK: 0.806725\n",
      "[75]\ttrain's QWK: 0.834648\tvalid's QWK: 0.820943\n",
      "[100]\ttrain's QWK: 0.839251\tvalid's QWK: 0.825566\n",
      "[125]\ttrain's QWK: 0.844151\tvalid's QWK: 0.829326\n",
      "[150]\ttrain's QWK: 0.847969\tvalid's QWK: 0.833209\n",
      "[175]\ttrain's QWK: 0.851064\tvalid's QWK: 0.834258\n",
      "[200]\ttrain's QWK: 0.853816\tvalid's QWK: 0.834701\n",
      "[225]\ttrain's QWK: 0.855818\tvalid's QWK: 0.835165\n",
      "[250]\ttrain's QWK: 0.858546\tvalid's QWK: 0.834564\n",
      "[275]\ttrain's QWK: 0.860886\tvalid's QWK: 0.834632\n",
      "[300]\ttrain's QWK: 0.862248\tvalid's QWK: 0.834893\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttrain's QWK: 0.856869\tvalid's QWK: 0.835694\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6572609079498505\n",
      "Cohen kappa score across fold: 0.8356943277817156\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.768255\tvalid's QWK: 0.756046\n",
      "[50]\ttrain's QWK: 0.824639\tvalid's QWK: 0.82316\n",
      "[75]\ttrain's QWK: 0.835673\tvalid's QWK: 0.834948\n",
      "[100]\ttrain's QWK: 0.841462\tvalid's QWK: 0.837461\n",
      "[125]\ttrain's QWK: 0.844285\tvalid's QWK: 0.837513\n",
      "[150]\ttrain's QWK: 0.848504\tvalid's QWK: 0.838085\n",
      "[175]\ttrain's QWK: 0.850384\tvalid's QWK: 0.839136\n",
      "[200]\ttrain's QWK: 0.853619\tvalid's QWK: 0.838489\n",
      "[225]\ttrain's QWK: 0.856932\tvalid's QWK: 0.838331\n",
      "[250]\ttrain's QWK: 0.859554\tvalid's QWK: 0.838285\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttrain's QWK: 0.851157\tvalid's QWK: 0.839708\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.675766794350908\n",
      "Cohen kappa score across fold: 0.8397080718440203\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.779019\tvalid's QWK: 0.760197\n",
      "[50]\ttrain's QWK: 0.828201\tvalid's QWK: 0.807\n",
      "[75]\ttrain's QWK: 0.837807\tvalid's QWK: 0.814672\n",
      "[100]\ttrain's QWK: 0.842126\tvalid's QWK: 0.818694\n",
      "[125]\ttrain's QWK: 0.845097\tvalid's QWK: 0.82239\n",
      "[150]\ttrain's QWK: 0.847908\tvalid's QWK: 0.823968\n",
      "[175]\ttrain's QWK: 0.849799\tvalid's QWK: 0.823921\n",
      "[200]\ttrain's QWK: 0.852449\tvalid's QWK: 0.824041\n",
      "[225]\ttrain's QWK: 0.854299\tvalid's QWK: 0.824141\n",
      "[250]\ttrain's QWK: 0.857537\tvalid's QWK: 0.823331\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[275]\ttrain's QWK: 0.860239\tvalid's QWK: 0.823328\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttrain's QWK: 0.852558\tvalid's QWK: 0.824742\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6687832985862595\n",
      "Cohen kappa score across fold: 0.824741807964359\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.780658\tvalid's QWK: 0.780743\n",
      "[50]\ttrain's QWK: 0.824896\tvalid's QWK: 0.825121\n",
      "[75]\ttrain's QWK: 0.836704\tvalid's QWK: 0.829745\n",
      "[100]\ttrain's QWK: 0.84166\tvalid's QWK: 0.830345\n",
      "[125]\ttrain's QWK: 0.844785\tvalid's QWK: 0.831542\n",
      "[150]\ttrain's QWK: 0.848939\tvalid's QWK: 0.831991\n",
      "[175]\ttrain's QWK: 0.851559\tvalid's QWK: 0.833939\n",
      "[200]\ttrain's QWK: 0.853552\tvalid's QWK: 0.835432\n",
      "[225]\ttrain's QWK: 0.856145\tvalid's QWK: 0.833749\n",
      "[250]\ttrain's QWK: 0.858189\tvalid's QWK: 0.834508\n",
      "Early stopping, best iteration is:\n",
      "[191]\ttrain's QWK: 0.853003\tvalid's QWK: 0.836026\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6712854959130883\n",
      "Cohen kappa score across fold: 0.8360259629506799\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.771305\tvalid's QWK: 0.780833\n",
      "[50]\ttrain's QWK: 0.824653\tvalid's QWK: 0.828263\n",
      "[75]\ttrain's QWK: 0.835047\tvalid's QWK: 0.835776\n",
      "[100]\ttrain's QWK: 0.840084\tvalid's QWK: 0.836893\n",
      "[125]\ttrain's QWK: 0.843283\tvalid's QWK: 0.837617\n",
      "[150]\ttrain's QWK: 0.845824\tvalid's QWK: 0.838255\n",
      "[175]\ttrain's QWK: 0.848319\tvalid's QWK: 0.839848\n",
      "[200]\ttrain's QWK: 0.851323\tvalid's QWK: 0.839037\n",
      "[225]\ttrain's QWK: 0.854017\tvalid's QWK: 0.839769\n",
      "[250]\ttrain's QWK: 0.856479\tvalid's QWK: 0.839028\n",
      "[275]\ttrain's QWK: 0.858768\tvalid's QWK: 0.83933\n",
      "Early stopping, best iteration is:\n",
      "[222]\ttrain's QWK: 0.853866\tvalid's QWK: 0.840304\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6774660574797903\n",
      "Cohen kappa score across fold: 0.8403041153398487\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "kappa_scores = []\n",
    "models = []\n",
    "predictions = []\n",
    "callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "feature_select = feature_select_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8743948f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:30:46.880173Z",
     "iopub.status.busy": "2024-06-01T11:30:46.879871Z",
     "iopub.status.idle": "2024-06-01T11:30:48.004210Z",
     "shell.execute_reply": "2024-06-01T11:30:48.003114Z"
    },
    "papermill": {
     "duration": 1.146163,
     "end_time": "2024-06-01T11:30:48.006252",
     "exception": false,
     "start_time": "2024-06-01T11:30:46.860089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Select Number:  13000\n"
     ]
    }
   ],
   "source": [
    "X = train_feats[feature_select].astype(np.float32).values\n",
    "print('Features Select Number: ', len(feature_select))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42abcc",
   "metadata": {
    "papermill": {
     "duration": 0.019288,
     "end_time": "2024-06-01T11:30:48.045963",
     "exception": false,
     "start_time": "2024-06-01T11:30:48.026675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train\n",
    "* I have trained and saved the model\n",
    "* you can choose to retrain or load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89cc03c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:30:48.087399Z",
     "iopub.status.busy": "2024-06-01T11:30:48.086987Z",
     "iopub.status.idle": "2024-06-01T11:57:08.008364Z",
     "shell.execute_reply": "2024-06-01T11:57:08.007249Z"
    },
    "papermill": {
     "duration": 1579.94494,
     "end_time": "2024-06-01T11:57:08.010895",
     "exception": false,
     "start_time": "2024-06-01T11:30:48.065955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.775048\tvalid's QWK: 0.754463\n",
      "[50]\ttrain's QWK: 0.821476\tvalid's QWK: 0.813628\n",
      "[75]\ttrain's QWK: 0.83309\tvalid's QWK: 0.827083\n",
      "[100]\ttrain's QWK: 0.838928\tvalid's QWK: 0.8303\n",
      "[125]\ttrain's QWK: 0.842761\tvalid's QWK: 0.832232\n",
      "[150]\ttrain's QWK: 0.845848\tvalid's QWK: 0.833415\n",
      "[175]\ttrain's QWK: 0.849212\tvalid's QWK: 0.833383\n",
      "[200]\ttrain's QWK: 0.851362\tvalid's QWK: 0.833299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[225]\ttrain's QWK: 0.85355\tvalid's QWK: 0.833756\n",
      "[250]\ttrain's QWK: 0.855118\tvalid's QWK: 0.836761\n",
      "[275]\ttrain's QWK: 0.856835\tvalid's QWK: 0.838011\n",
      "[300]\ttrain's QWK: 0.858627\tvalid's QWK: 0.838463\n",
      "[325]\ttrain's QWK: 0.860131\tvalid's QWK: 0.838776\n",
      "[350]\ttrain's QWK: 0.862112\tvalid's QWK: 0.838894\n",
      "[375]\ttrain's QWK: 0.8642\tvalid's QWK: 0.840366\n",
      "[400]\ttrain's QWK: 0.865442\tvalid's QWK: 0.84079\n",
      "[425]\ttrain's QWK: 0.866673\tvalid's QWK: 0.838685\n",
      "[450]\ttrain's QWK: 0.868215\tvalid's QWK: 0.836498\n",
      "[475]\ttrain's QWK: 0.870011\tvalid's QWK: 0.837526\n",
      "Early stopping, best iteration is:\n",
      "[403]\ttrain's QWK: 0.865813\tvalid's QWK: 0.842005\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6808935576520552\n",
      "Cohen kappa score across fold: 0.8420046954177576\n",
      "fold 2\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.771062\tvalid's QWK: 0.76669\n",
      "[50]\ttrain's QWK: 0.821911\tvalid's QWK: 0.81547\n",
      "[75]\ttrain's QWK: 0.833424\tvalid's QWK: 0.829074\n",
      "[100]\ttrain's QWK: 0.839782\tvalid's QWK: 0.832914\n",
      "[125]\ttrain's QWK: 0.843141\tvalid's QWK: 0.83381\n",
      "[150]\ttrain's QWK: 0.846592\tvalid's QWK: 0.831032\n",
      "[175]\ttrain's QWK: 0.848329\tvalid's QWK: 0.82956\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttrain's QWK: 0.843105\tvalid's QWK: 0.834335\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6526614535676233\n",
      "Cohen kappa score across fold: 0.8343350212378218\n",
      "fold 3\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.771275\tvalid's QWK: 0.763239\n",
      "[50]\ttrain's QWK: 0.82315\tvalid's QWK: 0.805093\n",
      "[75]\ttrain's QWK: 0.834618\tvalid's QWK: 0.817711\n",
      "[100]\ttrain's QWK: 0.840379\tvalid's QWK: 0.823874\n",
      "[125]\ttrain's QWK: 0.843713\tvalid's QWK: 0.827246\n",
      "[150]\ttrain's QWK: 0.846462\tvalid's QWK: 0.826819\n",
      "[175]\ttrain's QWK: 0.849145\tvalid's QWK: 0.83219\n",
      "[200]\ttrain's QWK: 0.851123\tvalid's QWK: 0.832289\n",
      "[225]\ttrain's QWK: 0.853317\tvalid's QWK: 0.833314\n",
      "[250]\ttrain's QWK: 0.854908\tvalid's QWK: 0.837255\n",
      "[275]\ttrain's QWK: 0.857058\tvalid's QWK: 0.839192\n",
      "[300]\ttrain's QWK: 0.859041\tvalid's QWK: 0.840185\n",
      "[325]\ttrain's QWK: 0.860582\tvalid's QWK: 0.841162\n",
      "[350]\ttrain's QWK: 0.862903\tvalid's QWK: 0.839257\n",
      "[375]\ttrain's QWK: 0.863913\tvalid's QWK: 0.840064\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttrain's QWK: 0.860614\tvalid's QWK: 0.84222\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6630770741620123\n",
      "Cohen kappa score across fold: 0.8422202979560056\n",
      "fold 4\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.778424\tvalid's QWK: 0.771448\n",
      "[50]\ttrain's QWK: 0.825873\tvalid's QWK: 0.819722\n",
      "[75]\ttrain's QWK: 0.835442\tvalid's QWK: 0.829122\n",
      "[100]\ttrain's QWK: 0.839811\tvalid's QWK: 0.832763\n",
      "[125]\ttrain's QWK: 0.844551\tvalid's QWK: 0.833755\n",
      "[150]\ttrain's QWK: 0.847184\tvalid's QWK: 0.833392\n",
      "[175]\ttrain's QWK: 0.848908\tvalid's QWK: 0.832012\n",
      "[200]\ttrain's QWK: 0.851614\tvalid's QWK: 0.831606\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttrain's QWK: 0.845476\tvalid's QWK: 0.834299\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6621107970225653\n",
      "Cohen kappa score across fold: 0.8342989801323282\n",
      "fold 5\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.774215\tvalid's QWK: 0.759499\n",
      "[50]\ttrain's QWK: 0.826575\tvalid's QWK: 0.808439\n",
      "[75]\ttrain's QWK: 0.835262\tvalid's QWK: 0.819295\n",
      "[100]\ttrain's QWK: 0.840438\tvalid's QWK: 0.823465\n",
      "[125]\ttrain's QWK: 0.843832\tvalid's QWK: 0.826754\n",
      "[150]\ttrain's QWK: 0.846776\tvalid's QWK: 0.827546\n",
      "[175]\ttrain's QWK: 0.848886\tvalid's QWK: 0.827503\n",
      "[200]\ttrain's QWK: 0.851208\tvalid's QWK: 0.826852\n",
      "[225]\ttrain's QWK: 0.853024\tvalid's QWK: 0.825665\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttrain's QWK: 0.848072\tvalid's QWK: 0.830421\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6672251423745181\n",
      "Cohen kappa score across fold: 0.8304208174544806\n",
      "fold 6\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.76818\tvalid's QWK: 0.76929\n",
      "[50]\ttrain's QWK: 0.821083\tvalid's QWK: 0.821387\n",
      "[75]\ttrain's QWK: 0.834623\tvalid's QWK: 0.831797\n",
      "[100]\ttrain's QWK: 0.840082\tvalid's QWK: 0.837001\n",
      "[125]\ttrain's QWK: 0.844187\tvalid's QWK: 0.840172\n",
      "[150]\ttrain's QWK: 0.846731\tvalid's QWK: 0.842243\n",
      "[175]\ttrain's QWK: 0.84877\tvalid's QWK: 0.844136\n",
      "[200]\ttrain's QWK: 0.851346\tvalid's QWK: 0.847176\n",
      "[225]\ttrain's QWK: 0.852369\tvalid's QWK: 0.846459\n",
      "[250]\ttrain's QWK: 0.854725\tvalid's QWK: 0.846105\n",
      "[275]\ttrain's QWK: 0.856665\tvalid's QWK: 0.848143\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\ttrain's QWK: 0.858266\tvalid's QWK: 0.848949\n",
      "[325]\ttrain's QWK: 0.860007\tvalid's QWK: 0.849861\n",
      "[350]\ttrain's QWK: 0.861624\tvalid's QWK: 0.850298\n",
      "[375]\ttrain's QWK: 0.863191\tvalid's QWK: 0.851656\n",
      "[400]\ttrain's QWK: 0.864997\tvalid's QWK: 0.849612\n",
      "[425]\ttrain's QWK: 0.866766\tvalid's QWK: 0.851526\n",
      "[450]\ttrain's QWK: 0.867893\tvalid's QWK: 0.851625\n",
      "[475]\ttrain's QWK: 0.868928\tvalid's QWK: 0.851852\n",
      "Early stopping, best iteration is:\n",
      "[410]\ttrain's QWK: 0.865555\tvalid's QWK: 0.853354\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.7014901316209644\n",
      "Cohen kappa score across fold: 0.8533541587458169\n",
      "fold 7\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.776812\tvalid's QWK: 0.767104\n",
      "[50]\ttrain's QWK: 0.821466\tvalid's QWK: 0.802596\n",
      "[75]\ttrain's QWK: 0.832522\tvalid's QWK: 0.815242\n",
      "[100]\ttrain's QWK: 0.838417\tvalid's QWK: 0.817399\n",
      "[125]\ttrain's QWK: 0.842151\tvalid's QWK: 0.821258\n",
      "[150]\ttrain's QWK: 0.845133\tvalid's QWK: 0.818292\n",
      "[175]\ttrain's QWK: 0.848959\tvalid's QWK: 0.821108\n",
      "[200]\ttrain's QWK: 0.851091\tvalid's QWK: 0.824206\n",
      "[225]\ttrain's QWK: 0.853495\tvalid's QWK: 0.823341\n",
      "[250]\ttrain's QWK: 0.855078\tvalid's QWK: 0.822761\n",
      "[275]\ttrain's QWK: 0.857087\tvalid's QWK: 0.823966\n",
      "[300]\ttrain's QWK: 0.858929\tvalid's QWK: 0.824595\n",
      "[325]\ttrain's QWK: 0.860096\tvalid's QWK: 0.825417\n",
      "[350]\ttrain's QWK: 0.861866\tvalid's QWK: 0.826931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[375]\ttrain's QWK: 0.86385\tvalid's QWK: 0.827286\n",
      "[400]\ttrain's QWK: 0.865314\tvalid's QWK: 0.827624\n",
      "[425]\ttrain's QWK: 0.867801\tvalid's QWK: 0.826501\n",
      "[450]\ttrain's QWK: 0.86971\tvalid's QWK: 0.828748\n",
      "[475]\ttrain's QWK: 0.870905\tvalid's QWK: 0.828409\n",
      "[500]\ttrain's QWK: 0.872418\tvalid's QWK: 0.829022\n",
      "[525]\ttrain's QWK: 0.87392\tvalid's QWK: 0.828576\n",
      "Early stopping, best iteration is:\n",
      "[451]\ttrain's QWK: 0.869598\tvalid's QWK: 0.829224\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6761432588468596\n",
      "Cohen kappa score across fold: 0.8292237961989352\n",
      "fold 8\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.776659\tvalid's QWK: 0.760569\n",
      "[50]\ttrain's QWK: 0.821988\tvalid's QWK: 0.808833\n",
      "[75]\ttrain's QWK: 0.833001\tvalid's QWK: 0.821001\n",
      "[100]\ttrain's QWK: 0.838546\tvalid's QWK: 0.826765\n",
      "[125]\ttrain's QWK: 0.843441\tvalid's QWK: 0.828573\n",
      "[150]\ttrain's QWK: 0.846347\tvalid's QWK: 0.828895\n",
      "[175]\ttrain's QWK: 0.849054\tvalid's QWK: 0.830323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\ttrain's QWK: 0.850774\tvalid's QWK: 0.830308\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[225]\ttrain's QWK: 0.852341\tvalid's QWK: 0.831733\n",
      "[250]\ttrain's QWK: 0.854468\tvalid's QWK: 0.830894\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[275]\ttrain's QWK: 0.856163\tvalid's QWK: 0.832805\n",
      "[300]\ttrain's QWK: 0.857778\tvalid's QWK: 0.833491\n",
      "[325]\ttrain's QWK: 0.859489\tvalid's QWK: 0.83499\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[350]\ttrain's QWK: 0.861257\tvalid's QWK: 0.834789\n",
      "[375]\ttrain's QWK: 0.862752\tvalid's QWK: 0.833539\n",
      "[400]\ttrain's QWK: 0.863725\tvalid's QWK: 0.834287\n",
      "[425]\ttrain's QWK: 0.865355\tvalid's QWK: 0.833477\n",
      "[450]\ttrain's QWK: 0.866769\tvalid's QWK: 0.834574\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttrain's QWK: 0.863397\tvalid's QWK: 0.836422\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6868020728926377\n",
      "Cohen kappa score across fold: 0.8364224000959747\n",
      "fold 9\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.782047\tvalid's QWK: 0.76623\n",
      "[50]\ttrain's QWK: 0.827095\tvalid's QWK: 0.806667\n",
      "[75]\ttrain's QWK: 0.834896\tvalid's QWK: 0.82058\n",
      "[100]\ttrain's QWK: 0.840518\tvalid's QWK: 0.825122\n",
      "[125]\ttrain's QWK: 0.844145\tvalid's QWK: 0.827857\n",
      "[150]\ttrain's QWK: 0.847175\tvalid's QWK: 0.831237\n",
      "[175]\ttrain's QWK: 0.849457\tvalid's QWK: 0.832818\n",
      "[200]\ttrain's QWK: 0.85167\tvalid's QWK: 0.832745\n",
      "[225]\ttrain's QWK: 0.853387\tvalid's QWK: 0.831672\n",
      "[250]\ttrain's QWK: 0.855181\tvalid's QWK: 0.832226\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttrain's QWK: 0.850224\tvalid's QWK: 0.833621\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6699813149076292\n",
      "Cohen kappa score across fold: 0.8336213363199277\n",
      "fold 10\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.770848\tvalid's QWK: 0.772705\n",
      "[50]\ttrain's QWK: 0.825403\tvalid's QWK: 0.82061\n",
      "[75]\ttrain's QWK: 0.835198\tvalid's QWK: 0.825507\n",
      "[100]\ttrain's QWK: 0.840294\tvalid's QWK: 0.82859\n",
      "[125]\ttrain's QWK: 0.843412\tvalid's QWK: 0.832445\n",
      "[150]\ttrain's QWK: 0.846198\tvalid's QWK: 0.837409\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[175]\ttrain's QWK: 0.848509\tvalid's QWK: 0.836285\n",
      "[200]\ttrain's QWK: 0.849992\tvalid's QWK: 0.836769\n",
      "Early stopping, best iteration is:\n",
      "[149]\ttrain's QWK: 0.845999\tvalid's QWK: 0.837878\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.672704875084069\n",
      "Cohen kappa score across fold: 0.837878493956316\n",
      "fold 11\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.780443\tvalid's QWK: 0.777947\n",
      "[50]\ttrain's QWK: 0.823141\tvalid's QWK: 0.819231\n",
      "[75]\ttrain's QWK: 0.832427\tvalid's QWK: 0.828976\n",
      "[100]\ttrain's QWK: 0.838872\tvalid's QWK: 0.832321\n",
      "[125]\ttrain's QWK: 0.8426\tvalid's QWK: 0.83481\n",
      "[150]\ttrain's QWK: 0.845887\tvalid's QWK: 0.837945\n",
      "[175]\ttrain's QWK: 0.848746\tvalid's QWK: 0.839046\n",
      "[200]\ttrain's QWK: 0.850479\tvalid's QWK: 0.839468\n",
      "[225]\ttrain's QWK: 0.852594\tvalid's QWK: 0.83835\n",
      "[250]\ttrain's QWK: 0.85409\tvalid's QWK: 0.839907\n",
      "Early stopping, best iteration is:\n",
      "[193]\ttrain's QWK: 0.849829\tvalid's QWK: 0.840586\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.680250718347006\n",
      "Cohen kappa score across fold: 0.8405862032215518\n",
      "fold 12\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.772936\tvalid's QWK: 0.76335\n",
      "[50]\ttrain's QWK: 0.821081\tvalid's QWK: 0.818961\n",
      "[75]\ttrain's QWK: 0.832914\tvalid's QWK: 0.827791\n",
      "[100]\ttrain's QWK: 0.839624\tvalid's QWK: 0.827196\n",
      "[125]\ttrain's QWK: 0.843275\tvalid's QWK: 0.827845\n",
      "[150]\ttrain's QWK: 0.845914\tvalid's QWK: 0.82817\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttrain's QWK: 0.837457\tvalid's QWK: 0.831226\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6613312300307337\n",
      "Cohen kappa score across fold: 0.8312255468932264\n",
      "fold 13\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.770631\tvalid's QWK: 0.771547\n",
      "[50]\ttrain's QWK: 0.823585\tvalid's QWK: 0.818606\n",
      "[75]\ttrain's QWK: 0.835403\tvalid's QWK: 0.824658\n",
      "[100]\ttrain's QWK: 0.840092\tvalid's QWK: 0.826295\n",
      "[125]\ttrain's QWK: 0.843023\tvalid's QWK: 0.824919\n",
      "[150]\ttrain's QWK: 0.846301\tvalid's QWK: 0.826901\n",
      "[175]\ttrain's QWK: 0.848881\tvalid's QWK: 0.82648\n",
      "[200]\ttrain's QWK: 0.851063\tvalid's QWK: 0.826092\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttrain's QWK: 0.845309\tvalid's QWK: 0.827821\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6575338858432588\n",
      "Cohen kappa score across fold: 0.8278214110652535\n",
      "fold 14\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.772086\tvalid's QWK: 0.787424\n",
      "[50]\ttrain's QWK: 0.821616\tvalid's QWK: 0.838634\n",
      "[75]\ttrain's QWK: 0.832285\tvalid's QWK: 0.845352\n",
      "[100]\ttrain's QWK: 0.837379\tvalid's QWK: 0.850434\n",
      "[125]\ttrain's QWK: 0.840796\tvalid's QWK: 0.852523\n",
      "[150]\ttrain's QWK: 0.844455\tvalid's QWK: 0.849591\n",
      "[175]\ttrain's QWK: 0.846878\tvalid's QWK: 0.851679\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttrain's QWK: 0.840761\tvalid's QWK: 0.85287\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.7023089081133681\n",
      "Cohen kappa score across fold: 0.8528700302926067\n",
      "fold 15\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.771587\tvalid's QWK: 0.795692\n",
      "[50]\ttrain's QWK: 0.819178\tvalid's QWK: 0.834409\n",
      "[75]\ttrain's QWK: 0.833005\tvalid's QWK: 0.843006\n",
      "[100]\ttrain's QWK: 0.838981\tvalid's QWK: 0.842592\n",
      "[125]\ttrain's QWK: 0.841549\tvalid's QWK: 0.843757\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttrain's QWK: 0.831804\tvalid's QWK: 0.845598\n",
      "Evaluated only: QWK\n",
      "F1 score across fold: 0.6924466590132489\n",
      "Cohen kappa score across fold: 0.8455983723458227\n"
     ]
    }
   ],
   "source": [
    "LOAD = False # re-train\n",
    "# Define the number of splits for cross-validation\n",
    "n_splits = 15\n",
    "models = []\n",
    "\n",
    "if LOAD:\n",
    "    for i in range(n_splits):\n",
    "        models.append(lgb.Booster(model_file=f'/kaggle/input/tfidf-lgbm-hisa/fold_{i+1}.txt'))\n",
    "else:\n",
    "    # Initialize StratifiedKFold with the specified number of splits\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    # Lists to store scores\n",
    "    f1_scores = []\n",
    "    kappa_scores = []\n",
    "    models = []\n",
    "    predictions = []\n",
    "    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "    # Loop through each fold of the cross-validation\n",
    "    i=1\n",
    "    for train_index, test_index in skf.split(X, y_split):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        print('fold',i)\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "                    objective = qwk_obj,\n",
    "                    metrics = 'None',\n",
    "                    learning_rate = 0.05,\n",
    "                    max_depth = 5,\n",
    "                    num_leaves = 10,\n",
    "                    colsample_bytree=0.3,\n",
    "                    reg_alpha = 0.7,\n",
    "                    reg_lambda = 0.1,\n",
    "                    n_estimators=700,\n",
    "                    random_state=42,\n",
    "                    extra_trees=True,\n",
    "                    class_weight='balanced',\n",
    "                    device='gpu' if CUDA_AVAILABLE else 'cpu',\n",
    "                    verbosity = - 1)\n",
    "\n",
    "        # Fit the model on the training data for this fold  \n",
    "        predictor = model.fit(X_train_fold,\n",
    "                              y_train_fold,\n",
    "                              eval_names=['train', 'valid'],\n",
    "                              eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n",
    "                              eval_metric=quadratic_weighted_kappa,\n",
    "                              callbacks=callbacks\n",
    "                             )\n",
    "\n",
    "        models.append(predictor)\n",
    "        # Make predictions on the test data for this fold\n",
    "        predictions_fold = predictor.predict(X_test_fold)\n",
    "        predictions_fold = predictions_fold + a\n",
    "        predictions_fold = predictions_fold.clip(1, 6).round()\n",
    "        predictions.append(predictions_fold)\n",
    "        # Calculate and store the F1 score for this fold\n",
    "        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n",
    "        f1_scores.append(f1_fold)\n",
    "\n",
    "        # Calculate and store the Cohen's kappa score for this fold\n",
    "        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n",
    "        kappa_scores.append(kappa_fold)\n",
    "        predictor.booster_.save_model(f'fold_{i}.txt')\n",
    "\n",
    "        print(f'F1 score across fold: {f1_fold}')\n",
    "        print(f'Cohen kappa score across fold: {kappa_fold}')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b477be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:57:08.090513Z",
     "iopub.status.busy": "2024-06-01T11:57:08.090186Z",
     "iopub.status.idle": "2024-06-01T11:57:08.097053Z",
     "shell.execute_reply": "2024-06-01T11:57:08.096259Z"
    },
    "papermill": {
     "duration": 0.047494,
     "end_time": "2024-06-01T11:57:08.098832",
     "exception": false,
     "start_time": "2024-06-01T11:57:08.051338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old code(2024-5-28)\n",
    "# LOAD = False\n",
    "# n_splits = 15\n",
    "# models = []\n",
    "# if LOAD:\n",
    "#     for i in range(5):\n",
    "#         models.append(lgb.Booster(model_file=f'../input/tfidf-lgbm-hisa/hisa_fold_{i}.txt'))\n",
    "        \n",
    "# else:\n",
    "#     # OOF is used to store the prediction results of each model on the validation set\n",
    "#     oof = []\n",
    "#     x= train_feats\n",
    "#     y= train_feats['score'].values\n",
    "#     # 15 fold\n",
    "#     kfold = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "#     callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "#     for fold_id, (trn_idx, val_idx) in tqdm(enumerate(kfold.split(x.copy(), y.copy().astype(str)))):\n",
    "#             # create model\n",
    "# #             model = lgb.LGBMRegressor(\n",
    "# #                 objective = qwk_obj,\n",
    "# #                 metrics = 'None',\n",
    "# #                 learning_rate = 0.1,\n",
    "# #                 max_depth = 5, \n",
    "# #                 num_leaves = 10,\n",
    "# #                 colsample_bytree=0.5,\n",
    "# #                 reg_alpha = 0.1, #L1\n",
    "# #                 reg_lambda = 0.8, #L2\n",
    "# #                 n_estimators=1024,\n",
    "# #                 random_state=42,\n",
    "# #                 verbosity = - 1)\n",
    "#             model = lgb.LGBMRegressor(\n",
    "#                 objective = qwk_obj,\n",
    "#                 metrics = 'None',\n",
    "#                 learning_rate = 0.05,\n",
    "#                 max_depth = 5,\n",
    "#                 num_leaves = 10,\n",
    "#                 colsample_bytree=0.3,\n",
    "#                 reg_alpha = 0.7,\n",
    "#                 reg_lambda = 0.1,\n",
    "#                 n_estimators=700,\n",
    "#                 random_state=42,\n",
    "#                 extra_trees=True,\n",
    "#                 class_weight='balanced',\n",
    "#                 verbosity = - 1)\n",
    "#             # Take out the training and validation sets for 5 kfold segmentation separately\n",
    "#             X_train = train_feats.iloc[trn_idx][feature_names]\n",
    "#             Y_train = train_feats.iloc[trn_idx]['score'] - a\n",
    "\n",
    "#             X_val = train_feats.iloc[val_idx][feature_names]\n",
    "#             Y_val = train_feats.iloc[val_idx]['score'] - a\n",
    "#             print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n",
    "#             # Training model\n",
    "#             lgb_model = model.fit(X_train,\n",
    "#                                   Y_train,\n",
    "#                                   eval_names=['train', 'valid'],\n",
    "#                                   eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "#                                   eval_metric=quadratic_weighted_kappa,\n",
    "#                                   callbacks=callbacks,)\n",
    "#             # Use the trained model to predict the validation set\n",
    "#             pred_val = lgb_model.predict(\n",
    "#                 X_val, num_iteration=lgb_model.best_iteration_)\n",
    "#             df_tmp = train_feats.iloc[val_idx][['essay_id', 'score']].copy()\n",
    "#             df_tmp['pred'] = pred_val + a\n",
    "#             oof.append(df_tmp)\n",
    "#             # Save model parameters\n",
    "#             models.append(model.booster_)\n",
    "#             lgb_model.booster_.save_model(f'hisa_fold_{fold_id}.txt')\n",
    "#     df_oof = pd.concat(oof)\n",
    "#     print(\"train end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97172e2",
   "metadata": {
    "papermill": {
     "duration": 0.037616,
     "end_time": "2024-06-01T11:57:08.174353",
     "exception": false,
     "start_time": "2024-06-01T11:57:08.136737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5be2e259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:57:08.252372Z",
     "iopub.status.busy": "2024-06-01T11:57:08.251803Z",
     "iopub.status.idle": "2024-06-01T11:57:08.258124Z",
     "shell.execute_reply": "2024-06-01T11:57:08.257143Z"
    },
    "papermill": {
     "duration": 0.047351,
     "end_time": "2024-06-01T11:57:08.259993",
     "exception": false,
     "start_time": "2024-06-01T11:57:08.212642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score across 15 folds: 0.6751307386319034\n",
      "Mean Cohen kappa score across 15 folds: 0.8381254374222549\n"
     ]
    }
   ],
   "source": [
    "# if LOAD:\n",
    "#     print('acc: ',0.6326919743456405)\n",
    "#     print('kappa: ',0.805136843120887)\n",
    "# else:\n",
    "#     acc = accuracy_score(df_oof['score'], df_oof['pred'].clip(1, 6).round())\n",
    "#     kappa = cohen_kappa_score(df_oof['score'], df_oof['pred'].clip(1, 6).round(), weights=\"quadratic\")\n",
    "#     print('acc: ',acc)\n",
    "#     print('kappa: ',kappa)\n",
    "    \n",
    "    \n",
    "if LOAD:\n",
    "    print(f'Mean F1 score across {n_splits} folds: 0.6694070084827064')\n",
    "    print(f'Mean Cohen kappa score across {n_splits} folds: 0.835342584985933')\n",
    "else:\n",
    "    # Calculate the mean scores across all folds\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "    mean_kappa_score = np.mean(kappa_scores)\n",
    "    # Print the mean scores\n",
    "    print(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\n",
    "    print(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deffb3a",
   "metadata": {
    "papermill": {
     "duration": 0.037738,
     "end_time": "2024-06-01T11:57:08.335489",
     "exception": false,
     "start_time": "2024-06-01T11:57:08.297751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c796d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:57:08.413365Z",
     "iopub.status.busy": "2024-06-01T11:57:08.412839Z",
     "iopub.status.idle": "2024-06-01T11:57:08.956024Z",
     "shell.execute_reply": "2024-06-01T11:57:08.955026Z"
    },
    "papermill": {
     "duration": 0.584658,
     "end_time": "2024-06-01T11:57:08.957985",
     "exception": false,
     "start_time": "2024-06-01T11:57:08.373327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features number:  21947\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_&gt;0_cnt</th>\n",
       "      <th>paragraph_&gt;50_cnt</th>\n",
       "      <th>paragraph_&gt;75_cnt</th>\n",
       "      <th>paragraph_&gt;100_cnt</th>\n",
       "      <th>paragraph_&gt;125_cnt</th>\n",
       "      <th>paragraph_&gt;150_cnt</th>\n",
       "      <th>paragraph_&gt;175_cnt</th>\n",
       "      <th>paragraph_&gt;200_cnt</th>\n",
       "      <th>paragraph_&gt;250_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_cnt_2208</th>\n",
       "      <th>tfid_cnt_2209</th>\n",
       "      <th>tfid_cnt_2210</th>\n",
       "      <th>tfid_cnt_2211</th>\n",
       "      <th>deberta_oof_0</th>\n",
       "      <th>deberta_oof_1</th>\n",
       "      <th>deberta_oof_2</th>\n",
       "      <th>deberta_oof_3</th>\n",
       "      <th>deberta_oof_4</th>\n",
       "      <th>deberta_oof_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.274048</td>\n",
       "      <td>0.693365</td>\n",
       "      <td>0.026059</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>0.911631</td>\n",
       "      <td>0.052944</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.018889</td>\n",
       "      <td>0.454450</td>\n",
       "      <td>0.515864</td>\n",
       "      <td>0.008228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21948 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_>0_cnt  paragraph_>50_cnt  paragraph_>75_cnt  \\\n",
       "0  000d118                 1                  1                  1   \n",
       "1  000fe60                 5                  5                  5   \n",
       "2  001ab80                 4                  4                  4   \n",
       "\n",
       "   paragraph_>100_cnt  paragraph_>125_cnt  paragraph_>150_cnt  \\\n",
       "0                   1                   1                   1   \n",
       "1                   5                   5                   5   \n",
       "2                   4                   4                   4   \n",
       "\n",
       "   paragraph_>175_cnt  paragraph_>200_cnt  paragraph_>250_cnt  ...  \\\n",
       "0                   1                   1                   1  ...   \n",
       "1                   5                   4                   3  ...   \n",
       "2                   4                   4                   4  ...   \n",
       "\n",
       "   tfid_cnt_2208  tfid_cnt_2209  tfid_cnt_2210  tfid_cnt_2211  deberta_oof_0  \\\n",
       "0              0              0              5              0       0.005867   \n",
       "1              0              0              0              0       0.000463   \n",
       "2              0              0              0              0       0.000969   \n",
       "\n",
       "   deberta_oof_1  deberta_oof_2  deberta_oof_3  deberta_oof_4  deberta_oof_5  \n",
       "0       0.274048       0.693365       0.026059       0.000465       0.000196  \n",
       "1       0.034625       0.911631       0.052944       0.000277       0.000060  \n",
       "2       0.001600       0.018889       0.454450       0.515864       0.008228  \n",
       "\n",
       "[3 rows x 21948 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#テストデータに対する前処理\n",
    "# Paragraph\n",
    "tmp = Paragraph_Preprocess(test)\n",
    "test_feats = Paragraph_Eng(tmp)\n",
    "# Sentence\n",
    "tmp = Sentence_Preprocess(test)\n",
    "test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "# Word\n",
    "tmp = Word_Preprocess(test)\n",
    "test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "# Tfidf\n",
    "test_tfid = vectorizer.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "# CountVectorizer\n",
    "test_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "# feature deberta\n",
    "for i in range(6):\n",
    "    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n",
    "\n",
    "# Features number\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n",
    "print('Features number: ',len(feature_names))\n",
    "test_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecb00a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:57:09.038795Z",
     "iopub.status.busy": "2024-06-01T11:57:09.037986Z",
     "iopub.status.idle": "2024-06-01T11:57:09.821154Z",
     "shell.execute_reply": "2024-06-01T11:57:09.820226Z"
    },
    "papermill": {
     "duration": 0.825616,
     "end_time": "2024-06-01T11:57:09.823438",
     "exception": false,
     "start_time": "2024-06-01T11:57:08.997822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 5.]\n"
     ]
    }
   ],
   "source": [
    "# old code(2024-5-28)\n",
    "# prediction = test_feats[['essay_id']].copy()\n",
    "# prediction['score'] = 0\n",
    "# pred_test = models[0].predict(test_feats[feature_names]) + a\n",
    "# for i in range(4):\n",
    "#     pred_now = models[i+1].predict(test_feats[feature_names]) + a\n",
    "#     pred_test = np.add(pred_test,pred_now)\n",
    "# # The final prediction result needs to be divided by 5 because the prediction results of 5 models were added together\n",
    "# pred_test = pred_test/15\n",
    "# print(pred_test)\n",
    "\n",
    "probabilities = []\n",
    "for model in models:\n",
    "    proba = model.predict(test_feats[feature_select]) + a\n",
    "    probabilities.append(proba)\n",
    "    \n",
    "# Compute the average probabilities across all models\n",
    "predictions = np.mean(probabilities, axis=0)\n",
    "predictions = np.round(predictions.clip(1, 6))\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bfb67de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T11:57:09.905030Z",
     "iopub.status.busy": "2024-06-01T11:57:09.904371Z",
     "iopub.status.idle": "2024-06-01T11:57:09.926072Z",
     "shell.execute_reply": "2024-06-01T11:57:09.925205Z"
    },
    "papermill": {
     "duration": 0.063876,
     "end_time": "2024-06-01T11:57:09.928064",
     "exception": false,
     "start_time": "2024-06-01T11:57:09.864188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  score\n",
       "0  000d118      2\n",
       "1  000fe60      3\n",
       "2  001ab80      5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# old code(2024-5-28)\n",
    "# Round the prediction result to an integer and limit it to a range of 1-6 (score range)\n",
    "# pred_test = pred_test.clip(1, 6).round()\n",
    "# prediction['score'] = pred_test\n",
    "# prediction.to_csv('submission.csv', index=False)\n",
    "# prediction.head(3)\n",
    "\n",
    "submission = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n",
    "submission['score'] = predictions\n",
    "submission['score'] = submission['score'].astype(int)\n",
    "submission.to_csv(\"submission.csv\", index=None)\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b5919",
   "metadata": {
    "papermill": {
     "duration": 0.038723,
     "end_time": "2024-06-01T11:57:10.005829",
     "exception": false,
     "start_time": "2024-06-01T11:57:09.967106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reference Notebook\n",
    "#### I would like to give thanks to the authors of these public notebooks. I have learned a lot from you.\n",
    "* https://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n",
    "* https://www.kaggle.com/code/yunsuxiaozi/aes2-0-baseline-naivebayesclassifier\n",
    "* https://www.kaggle.com/code/finlay/llm-detect-0-to-1\n",
    "* https://www.kaggle.com/code/awqatak/silver-bullet-single-model-165-features\n",
    "* https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features\n",
    "* https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 4813598,
     "sourceId": 8141507,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4832208,
     "sourceId": 8166166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4902588,
     "sourceId": 8260229,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 180541943,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4183.111674,
   "end_time": "2024-06-01T11:57:13.439461",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-01T10:47:30.327787",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "18f11b8be9524989891ec54570cd561c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2db4be224031463e9d9b7c6114e4d1ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c93ee2a03674b5b999dfa6cc5911407",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b9c14ae8818642038daa17b0f6d249a0",
       "value": 3.0
      }
     },
     "45eb8909e2b94f349fbdf01549c57dae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c93ee2a03674b5b999dfa6cc5911407": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b655b55ddd24360a5ef8edab8eb3a9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "851be7996f5c4f03b58bbabc4542b5c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f536dec02bb042f8948e36d55fdf3223",
       "placeholder": "​",
       "style": "IPY_MODEL_a535ed8b6b5146df96804974e392265e",
       "value": "100%"
      }
     },
     "a444c78c204c49fbaa60d23a8b65a9a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b655b55ddd24360a5ef8edab8eb3a9f",
       "placeholder": "​",
       "style": "IPY_MODEL_45eb8909e2b94f349fbdf01549c57dae",
       "value": " 3/3 [00:00&lt;00:00, 116.05ex/s]"
      }
     },
     "a535ed8b6b5146df96804974e392265e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b9c14ae8818642038daa17b0f6d249a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f536dec02bb042f8948e36d55fdf3223": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff82cceae3cd4432b514746baec5bcf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_851be7996f5c4f03b58bbabc4542b5c6",
        "IPY_MODEL_2db4be224031463e9d9b7c6114e4d1ee",
        "IPY_MODEL_a444c78c204c49fbaa60d23a8b65a9a4"
       ],
       "layout": "IPY_MODEL_18f11b8be9524989891ec54570cd561c"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
