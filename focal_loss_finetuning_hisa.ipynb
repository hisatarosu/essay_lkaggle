{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8474123,"sourceType":"datasetVersion","datasetId":5053329},{"sourceId":179034686,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport gc\nimport re\n# from datasets import Dataset\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets.arrow_dataset import Dataset\nfrom transformers.trainer import Trainer\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.trainer_utils import EvalPrediction\nfrom transformers.training_args import TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom tokenizers import AddedToken\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, matthews_corrcoef, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T11:30:07.647638Z","iopub.execute_input":"2024-05-21T11:30:07.648024Z","iopub.status.idle":"2024-05-21T11:30:07.655138Z","shell.execute_reply.started":"2024-05-21T11:30:07.647993Z","shell.execute_reply":"2024-05-21T11:30:07.654018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    checkpoint: str = \"microsoft/deberta-v3-base\"\n    per_device_train_batch_size: int = 4\n    per_device_eval_batch_size: int = 8\n    gradient_accumulation_steps: int = 8 // torch.cuda.device_count() / per_device_train_batch_size\n    num_train_epochs: float = 4 #sample 4\n    train_max_length: int = 1024\n    eval_max_length: int = 2048\n    lr: float = 1e-5\n    scheduler: str = \"linear\"\n    warmup_ratio: float = 0.0\n    weight_decay = 0.01\n    amp: bool = True\n    n_splits: int = 5\n    gamma: float = 2.\n    optim: str = \"adamw_torch\"\n    inference: bool = True#False:train True:inference\n    inference_checkpoints_dir: str = \"/kaggle/input/focal-loss-finetuning-hisa/output/\"\n    \nconfig = Config()\nprint('shoq_config:',config)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:51.501796Z","iopub.execute_input":"2024-05-21T11:26:51.502347Z","iopub.status.idle":"2024-05-21T11:26:51.546971Z","shell.execute_reply.started":"2024-05-21T11:26:51.502319Z","shell.execute_reply":"2024-05-21T11:26:51.546032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"output\",\n    report_to=\"none\",\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    num_train_epochs=config.num_train_epochs,\n    weight_decay=config.weight_decay,\n    evaluation_strategy='epoch',\n    save_strategy=\"epoch\",\n    logging_steps=10,\n    save_total_limit=1,\n    metric_for_best_model=\"qwk\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    fp16=config.amp,\n    learning_rate=config.lr,\n    lr_scheduler_type=config.scheduler,\n    warmup_ratio=config.warmup_ratio,\n    optim=config.optim #\"adamw_torch\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:51.548211Z","iopub.execute_input":"2024-05-21T11:26:51.548619Z","iopub.status.idle":"2024-05-21T11:26:51.644130Z","shell.execute_reply.started":"2024-05-21T11:26:51.548585Z","shell.execute_reply":"2024-05-21T11:26:51.643279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiate the model & tokenizer","metadata":{}},{"cell_type":"code","source":"class ModelInit:\n    model_class = AutoModelForSequenceClassification\n    \n    def __init__(self, checkpoint: str, num_labels: int = 6) -> None:\n        self.model = self.model_class.from_pretrained(checkpoint, num_labels=num_labels)\n        self.state_dict = copy.deepcopy(self.model.state_dict())\n        \n    def __call__(self) -> model_class:\n        self.model.load_state_dict(self.state_dict)\n        return self.model","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:51.646582Z","iopub.execute_input":"2024-05-21T11:26:51.647237Z","iopub.status.idle":"2024-05-21T11:26:51.653176Z","shell.execute_reply.started":"2024-05-21T11:26:51.647200Z","shell.execute_reply":"2024-05-21T11:26:51.652264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiate the dataset","metadata":{}},{"cell_type":"code","source":"if config.inference:\n    df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n    print('read test.csv')\nelse:\n    df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n    print('read train.csv')\nds = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:51.654691Z","iopub.execute_input":"2024-05-21T11:26:51.655054Z","iopub.status.idle":"2024-05-21T11:26:51.735566Z","shell.execute_reply.started":"2024-05-21T11:26:51.655023Z","shell.execute_reply":"2024-05-21T11:26:51.734669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder:\n    def __init__(self, tokenizer, **encode_kwargs):\n        self.tokenizer = tokenizer\n        self.kwargs = encode_kwargs\n        \n    def __call__(self, batch: dict) -> dict:\n        encoded = self.tokenizer(batch[\"full_text\"], **self.kwargs)\n        encoded[\"labels\"] = [s-1 for s in batch[\"score\"]]  # score is 1~6\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:51.736671Z","iopub.execute_input":"2024-05-21T11:26:51.736956Z","iopub.status.idle":"2024-05-21T11:26:51.742647Z","shell.execute_reply.started":"2024-05-21T11:26:51.736930Z","shell.execute_reply":"2024-05-21T11:26:51.741695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute Metrics","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred: EvalPrediction) -> dict:\n    predictions = eval_pred.predictions\n    y_true = eval_pred.label_ids\n    y_pred = predictions.argmax(-1)\n    kappa = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    corr = matthews_corrcoef(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    return {\"qwk\": kappa, \"corr\": corr, \"acc\": acc}","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:51.743836Z","iopub.execute_input":"2024-05-21T11:26:51.744117Z","iopub.status.idle":"2024-05-21T11:26:51.753613Z","shell.execute_reply.started":"2024-05-21T11:26:51.744094Z","shell.execute_reply":"2024-05-21T11:26:51.752730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Trainer with Focal Loss","metadata":{}},{"cell_type":"code","source":"class FocalLoss(torch.nn.Module):\n    def __init__(self, weight: torch.Tensor | None = None, gamma: float = 2,) -> None:\n        super().__init__()\n        self.ce = torch.nn.CrossEntropyLoss(weight=weight)\n        self.gamma = gamma\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        ce_loss: torch.Tensor = self.ce(input, target)\n        pt = torch.exp(-ce_loss)\n        f_loss = (1 - pt) ** self.gamma * ce_loss\n        f_loss = torch.mean(f_loss)\n        return f_loss\n    \n    \nclass FocalLossTrainer(Trainer):\n    def compute_loss(self, model: PreTrainedModel, inputs: dict, return_outputs: bool = False) -> tuple:\n        ce_loss, outputs = super().compute_loss(model, inputs, True)\n        labels = inputs[\"labels\"]\n        logits = outputs[\"logits\"]\n        loss_fn = FocalLoss(gamma=config.gamma)\n        loss = loss_fn(input=logits, target=labels)\n        outputs[\"loss\"] = loss\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:26:57.488968Z","iopub.execute_input":"2024-05-21T11:26:57.489801Z","iopub.status.idle":"2024-05-21T11:26:57.498478Z","shell.execute_reply.started":"2024-05-21T11:26:57.489768Z","shell.execute_reply":"2024-05-21T11:26:57.497568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features engineering","metadata":{}},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\n# def count_spelling_errors(text):\n#     doc = nlp(text)\n#     lemmatized_tokens = [token.lemma_.lower() for token in doc]\n#     spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n#     return spelling_errors\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = expandContractions(x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\n# 前処理の適用\ndf[\"full_text\"] = df[\"full_text\"] .apply(dataPreprocessing)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:30:12.928378Z","iopub.execute_input":"2024-05-21T11:30:12.929086Z","iopub.status.idle":"2024-05-21T11:30:13.210658Z","shell.execute_reply.started":"2024-05-21T11:30:12.929055Z","shell.execute_reply":"2024-05-21T11:30:13.209653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{"execution":{"iopub.status.busy":"2024-04-26T09:44:29.178919Z","iopub.execute_input":"2024-04-26T09:44:29.179293Z","iopub.status.idle":"2024-04-26T09:44:29.210433Z","shell.execute_reply.started":"2024-04-26T09:44:29.17926Z","shell.execute_reply":"2024-04-26T09:44:29.209755Z"}}},{"cell_type":"code","source":"if not config.inference:\n    model = ModelInit(config.checkpoint)\n    tokenizer = AutoTokenizer.from_pretrained(config.checkpoint)\n    tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\n    tokenizer.add_tokens([AddedToken(\" \"*2, normalized=False)])\n    \n    train_encoder = Encoder(tokenizer, max_length=config.train_max_length, truncation=True)\n    eval_encoder = Encoder(tokenizer, max_length=config.eval_max_length, truncation=True)\n    \n    # 5-fold stratified cv\n    cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=42)\n    folds = list(cv.split(np.zeros(len(df)), y=df[\"score\"].values))\n    idx2fold = {idx: fold for fold, (_, val_idx) in enumerate(folds) for idx in val_idx}\n    df[\"fold\"] = [idx2fold[i] for i in df.index]\n    # 'fold'列が正しく作成されているか確認\n    print('fold_split',df.head())\n    df.to_csv(\"train_split.csv\", index=False)\n    \n\n    # データフレームをHugging Faceデータセットに変換\n    ds = Dataset.from_pandas(df)\n\n    # データセットに'fold'列が含まれているか確認\n    print('ds.features:',ds.features)\n    print(ds[0])\n    \n    cv_res = []\n    \n    \n    for fold_idx in sorted(df[\"fold\"].unique()):\n        args.output_dir = os.path.join(\"output\", f\"fold_{fold_idx}\")\n        args.run_name = f\"{config.checkpoint}_fold-{fold_idx}\"\n        train_ds = ds.select([i for i, d in enumerate(ds) if d[\"fold\"] != fold_idx])\n        eval_ds = ds.select([i for i, d in enumerate(ds) if d[\"fold\"] == fold_idx])\n        train_ds = train_ds.map(train_encoder, batched=True)\n        eval_ds = eval_ds.map(eval_encoder, batched=True)\n        #Focalloss（損失関数）\n        trainer = FocalLossTrainer(\n            args=args, \n            train_dataset=train_ds, \n            eval_dataset=eval_ds,\n            tokenizer=tokenizer,\n            model_init=model,\n            compute_metrics=compute_metrics,\n        )\n        trainer.train()\n        preds = trainer.predict(eval_ds).predictions\n        qwk = cohen_kappa_score(y1=np.array(eval_ds[\"labels\"]), y2=preds.argmax(-1), weights=\"quadratic\")\n        fig, ax = plt.subplots()\n        ConfusionMatrixDisplay.from_predictions(\n            y_true=np.array(eval_ds[\"labels\"]), \n            y_pred=preds.argmax(-1),\n            ax=ax\n        )\n        ax.set_title(f\"fold-{fold_idx} qwk: {qwk:.3f}\")\n        fig.show()\n        cv_res.append(qwk)\n\n        # foldごとにモデルを保存\n        model_path = f'./finetuning_deberta_fold_{fold_idx}'\n        Path(model_path).mkdir(parents=True, exist_ok=True)\n        trainer.model.save_pretrained(model_path)\n        tokenizer.save_pretrained(model_path)\n        \n        # メモリ解放\n        del trainer\n        del train_ds\n        del eval_ds\n        del preds\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        \n\n    # foldとcv_resの長さを確認\n    print(f\"Number of folds: {len(sorted(df['fold'].unique()))}\")\n    print(f\"Number of cv_res: {len(cv_res)}\")\n    res_df = pd.DataFrame(\n        {\n            \"fold\": list(sorted(df[\"fold\"].unique())) + [\"mean\"],\n            \"qwk\": cv_res + [np.mean(cv_res)]\n        }\n    )\n    print(res_df)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-21T11:30:30.234925Z","iopub.execute_input":"2024-05-21T11:30:30.235830Z","iopub.status.idle":"2024-05-21T11:44:47.768477Z","shell.execute_reply.started":"2024-05-21T11:30:30.235795Z","shell.execute_reply":"2024-05-21T11:44:47.767302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config.inference:\n    predictions = 0\n    checkpoints = list(Path(config.inference_checkpoints_dir).glob(\"fold*/checkpoint*\"))\n    print(checkpoints)\n\n    for checkpoint in checkpoints:\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(checkpoint)\n        model = DebertaV2ForSequenceClassification.from_pretrained(checkpoint)\n        _ds = ds.map(\n            lambda i: tokenizer(i[\"full_text\"], max_length=config.eval_max_length, truncation=True), \n            batched=True,\n        )\n        args = TrainingArguments(\n            output_dir=\".\",\n            per_device_eval_batch_size=config.per_device_eval_batch_size,\n            fp16=config.amp,\n        )\n        trainer = Trainer(args=args, model=model, tokenizer=tokenizer)\n        preds = trainer.predict(_ds)\n        predictions += preds.predictions / len(checkpoints)\n\n    predicted_scores = predictions.argmax(-1) + 1  # [0,5] -> [1,6]\n    \n    df[\"score\"] = predicted_scores\n    df = df[[\"essay_id\", \"score\"]]\n    display(df)\n    df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:25:18.161858Z","iopub.status.idle":"2024-05-19T13:25:18.162264Z","shell.execute_reply.started":"2024-05-19T13:25:18.162071Z","shell.execute_reply":"2024-05-19T13:25:18.162087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}